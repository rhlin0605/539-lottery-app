import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from collections import Counter
from itertools import combinations
import random
from datetime import datetime

# é é¢è¨­å®š
st.set_page_config(page_title="ä»Šå½©539é æ¸¬ç³»çµ±", layout="wide")
st.title("ğŸ¯ ä»Šå½©539é æ¸¬ç³»çµ±ï¼ˆè‡ªå‹•è£œç¼ºè³‡æ–™+çµ±è¨ˆ+é æ¸¬ï¼‰")

local_csv = "539_data.csv"

# è®€å–ç¾æœ‰ CSV
try:
    local_df = pd.read_csv(local_csv, encoding='utf-8')
except FileNotFoundError:
    local_df = pd.DataFrame(columns=['Date', 'NO.1', 'NO.2', 'NO.3', 'NO.4', 'NO.5'])

# Sidebar è¨­å®š
debug_mode = st.sidebar.checkbox("ğŸ”§ Debug Mode", value=False)
num_fetch = st.sidebar.number_input("æŠ“å–æœ€æ–°NæœŸï¼ˆç¶²ç«™è³‡æ–™ï¼‰", 1, 100, 50)

# å–å¾—è³‡æ–™ä¾†æº
def fetch_from_primary_source(num_fetch=50, debug=False):
    url = 'https://www.pilio.idv.tw/lto539/list.asp'
    latest_rows = []
    try:
        resp = requests.get(url, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            has_header = False
            if rows and len(rows[0].find_all('td')) >= 2:
                first_row_text = [c.get_text(strip=True) for c in rows[0].find_all('td')]
                if any('æ—¥æœŸ' in text or 'ä»Šå½©' in text for text in first_row_text):
                    has_header = True
            start_idx = 1 if has_header else 0
            for row in rows[start_idx:start_idx+num_fetch]:
                cols = row.find_all('td')
                if len(cols) >= 2 and '/' in cols[0].get_text():
                    try:
                        date = cols[0].get_text(strip=True).split('(')[0]
                        numbers_text = cols[1].get_text(strip=True).replace('\xa0', '')
                        numbers = [int(x) for x in numbers_text.split(',')]
                        latest_rows.append([date] + numbers)
                    except Exception as e:
                        if debug:
                            st.warning(f"âš ï¸ Pilioè§£æéŒ¯èª¤: {e}")
        return latest_rows
    except Exception as e:
        st.warning(f"âš ï¸ ä¸»è³‡æ–™æºæŠ“å–å¤±æ•—ï¼š{e}")
        return []

def fetch_from_secondary_source(debug=False):
    url = 'https://lotto.ctbcbank.com/result_all.htm#07'
    latest_rows = []
    try:
        resp = requests.get(url, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        rows = soup.find_all('tr')
        for row in rows:
            cols = row.find_all('td')
            if len(cols) >= 5:
                try:
                    date_text = cols[0].get_text(strip=True)
                    if '/' in date_text:
                        date = date_text.split(' ')[0]
                        numbers_text = cols[4].get_text(strip=True).replace('\xa0', '')
                        numbers = [int(x) for x in numbers_text.split()]
                        latest_rows.append([date] + numbers)
                except Exception as e:
                    if debug:
                        st.warning(f"âš ï¸ CTBCè§£æéŒ¯èª¤: {e}")
        return latest_rows
    except Exception as e:
        st.warning(f"âš ï¸ å‚™ç”¨è³‡æ–™æºæŠ“å–å¤±æ•—ï¼š{e}")
        return []

# Debug Modeï¼šå³æ™‚é¡¯ç¤ºè³‡æ–™
if st.sidebar.button("ğŸ› ï¸ ç«‹å³æŠ“å–è³‡æ–™ï¼ˆDebugï¼‰"):
    st.subheader("ğŸ” Debug Modeï¼šç«‹å³æŠ“å–è³‡æ–™")
    pilio_data = fetch_from_primary_source(num_fetch, debug=True)
    ctbcbank_data = fetch_from_secondary_source(debug=True)
    st.write("âœ… Primary Sourceï¼ˆPilioï¼‰è³‡æ–™ï¼ˆå‰5ç­†ï¼‰:")
    st.write(pilio_data[:5])
    st.write("âœ… Secondary Sourceï¼ˆCTBCï¼‰è³‡æ–™ï¼ˆå‰5ç­†ï¼‰:")
    st.write(ctbcbank_data[:5])

# è‡ªå‹•æŠ“å–æµç¨‹
latest_rows = fetch_from_primary_source(num_fetch, debug=debug_mode)
if not latest_rows:
    if debug_mode:
        st.info("ğŸ”„ Primary Source æŠ“å–å¤±æ•—æˆ–ç„¡è³‡æ–™ï¼Œå˜—è©¦ä½¿ç”¨å‚™ç”¨è³‡æ–™æº...")
    latest_rows = fetch_from_secondary_source(debug=debug_mode)

if latest_rows:
    latest_df = pd.DataFrame(latest_rows, columns=['Date', 'NO.1', 'NO.2', 'NO.3', 'NO.4', 'NO.5'])
    if debug_mode:
        st.write("ğŸ—‚ï¸ æœ€æ–°è³‡æ–™ï¼ˆç¬¬ä¸€ç­†ï¼‰:", latest_df.head(1))
    if not local_df.empty:
        existing_dates = set(local_df['Date'].astype(str))
        new_rows = latest_df[~latest_df['Date'].astype(str).isin(existing_dates)]
    else:
        new_rows = latest_df
    if not new_rows.empty:
        local_df = pd.concat([new_rows, local_df], ignore_index=True)
        local_df.drop_duplicates(subset=['Date'], inplace=True)
        local_df.sort_values(by='Date', ascending=False, inplace=True)
        local_df.to_csv(local_csv, index=False, encoding='utf-8')
        st.success(f"âœ… è³‡æ–™åº«å·²è£œä¸Š {len(new_rows)} ç­†æ–°è³‡æ–™ï¼Œå…± {len(local_df)} æœŸ")
    else:
        st.info("ğŸ“… è³‡æ–™åº«å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°ã€‚")
else:
    st.error("âŒ ç„¡æ³•å¾ä»»ä¸€è³‡æ–™æºå–å¾—è³‡æ–™ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")

# é¡¯ç¤ºæœ€æ–°è³‡æ–™
st.subheader("ğŸ“… æœ€æ–°è³‡æ–™ï¼ˆå‰ 5 ç­†ï¼‰")
st.dataframe(local_df.head(5))

# å¾ŒçºŒçµ±è¨ˆåˆ†æã€æ¬Šé‡è¨­å®šã€é æ¸¬é¸è™Ÿç­‰åŠŸèƒ½ï¼ˆè«‹æ¥ä¸Šä¹‹å‰å®Œæ•´ç‰ˆæœ¬ï¼‰
# é€™è£¡å°±ä¿ç•™ä¹‹å‰çš„çµ±è¨ˆã€é æ¸¬ã€æ¨¡æ“¬ã€CSVä¸‹è¼‰çš„é‚è¼¯ï¼Œä¸éœ€è¦æ›´å‹•

# ç¯„ä¾‹ï¼šé¡¯ç¤º Debug Mode ç•«é¢
if debug_mode:
    st.info("ğŸ” Debug Mode é–‹å•Ÿä¸­ï¼šç³»çµ±å°‡é¡¯ç¤ºè³‡æ–™ä¾†æºçš„è§£æç´°ç¯€ã€‚")

# ... (çµ±è¨ˆåˆ†æã€æ¬Šé‡è¨­å®šã€é æ¸¬é¸è™Ÿè«‹ä¾ç…§æ‚¨ç›®å‰å®Œæ•´çš„ç¨‹å¼ç¢¼æ¥çºŒ)
